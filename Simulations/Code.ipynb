{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.stats import pareto\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.io\n",
    "import collections\n",
    "import pickle\n",
    "from mpl_toolkits.axes_grid1.inset_locator import (inset_axes,InsetPosition,mark_inset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Comparing MSE of the two methods (Figure 2 of the manuscript)\n",
    "\n",
    "### Description of the Code:  This code compares the theoretical MSE of the vanilla method with that of the proposed friendship paradox based method using the derived expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.linspace(2.1, 3.5, num=3000)\n",
    "n = 100\n",
    "\n",
    "Bias_X_theoretical = ( (alpha - 1) / (n-1) )\n",
    "Var_X_theoretical =  ( (n**2) * ((alpha - 1)**2) )  / ( ((n-1)**2) * (n-2) )\n",
    "MSE_X_theoretical = Bias_X_theoretical**2 + Var_X_theoretical\n",
    "\n",
    "Bias_Y_theoretical = ( (alpha - 2) / (n-1) )\n",
    "Var_Y_theoretical =  ( (n**2) * ((alpha - 2)**2) )  / ( ((n-1)**2) * (n-2) )\n",
    "MSE_Y_theoretical = Bias_Y_theoretical**2 + Var_Y_theoretical\n",
    "\n",
    "\n",
    "# Plotting the MSE values of the two estimates\n",
    "plt.figure(figsize=(2.75, 2.3))\n",
    "plt.xlabel(r'Power-law exponent $\\alpha$', fontsize=7)\n",
    "plt.ylabel(r'Theoretical MSE', fontsize=7)\n",
    "plt.xlim((2.05, 3.55))\n",
    "plt.xticks(np.arange(2.1, 3.61, step=0.2), fontsize=7)\n",
    "plt.yticks(fontsize=7)\n",
    "\n",
    "plot_MSE_X_theoretical = plt.plot(alpha, MSE_X_theoretical,\n",
    "                      label=r'$\\mathrm{MSE}\\{\\hat{\\alpha}_{\\mathrm{vl}}\\},\\, n = 100$')\n",
    "\n",
    "\n",
    "plot_MSE_Y_theoretical = plt.plot(alpha, MSE_Y_theoretical,\n",
    "                      label=r'$\\mathrm{MSE}\\{\\hat{\\alpha}_{\\mathrm{fp}}\\},\\, n = 100$'\n",
    "                                 )\n",
    "plt.legend(ncol=1, loc='upper left', fontsize=7, edgecolor = 'inherit')\n",
    "plt.savefig('MSE_X_vs_MSE_Y.pdf', bbox_inches='tight')\n",
    "\n",
    "# Plotting the ratio of the MSE values of the two estimates\n",
    "plt.figure(figsize=(2.75, 2.3))\n",
    "plt.xlabel(r'Power-law exponent $\\alpha$', fontsize=7)\n",
    "plt.ylabel(r'Ratio of Theoretical MSEs', fontsize=7)\n",
    "plt.xlim((2.05, 3.55))\n",
    "plt.xticks(np.arange(2.1, 3.61, step=0.2), fontsize=7)\n",
    "plt.yticks(np.arange(0, 130, step=20), fontsize=7)\n",
    "plt.yticks(fontsize=7)\n",
    "plt.yscale('log')\n",
    "\n",
    "plot_MSE_Ratio_theoretical = plt.plot(alpha, MSE_X_theoretical/MSE_Y_theoretical,\n",
    "                      label=r'$\\frac{\\mathrm{MSE}\\{\\hat{\\alpha}_{\\mathrm{vl}}\\}}{\\mathrm{MSE}\\{\\hat{\\alpha}_{\\mathrm{fp}}\\}}$'\n",
    "                                     )\n",
    "plt.legend(ncol=1, loc='upper right', fontsize=10, edgecolor = 'inherit')\n",
    "\n",
    "plt.savefig('MSE_X_MSE_Y_ratio.pdf', bbox_inches='tight')                                      \n",
    "                                      \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Code: This code numerically estimate and store the:\n",
    "##   1: Bias\n",
    "##   2: Variance \n",
    "##   3: Mean-squared error (sum of squared bias and variance)\n",
    "##   4: Cramer-Rao Lower Bound (best achievable variance for any unbiased estimate),\n",
    "## of the four estimates considered in the paper for different values of the power-law exponent and the minimum degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluting the bias, variance, MSE values for difference sample sizes\n",
    "\n",
    "random.seed(123)  # Initializing the random number generators\n",
    "\n",
    "No_of_Nodes = 50000  # Number of nodes in each of the power-law graphs\n",
    "No_iterations = 5000  # Number of independent iterations in the Monte-Carlo simulation\n",
    "k_min = 500 # Minimum degree\n",
    "\n",
    "#Dictionary for storing the results for various n values\n",
    "Dict_Results = {}\n",
    "\n",
    "# The following for loop considers four different values of the sample size\n",
    "for n in np.arange(50, 250, 50):\n",
    "    # Lists for storing the bias and variance for each value of alpha for a given value of sample size n\n",
    "    Bias2_Var_MSE_X_vec = []\n",
    "    Bias2_Var_MSE_X_discrete_vec = []\n",
    "    Bias2_Var_MSE_Y_vec = []\n",
    "    Bias2_Var_MSE_Y_discrete_vec = []\n",
    "    \n",
    "    for alpha in np.linspace(2.1, 3.5, num=25):\n",
    "        i = 1  # i is the current iteration number\n",
    "\n",
    "        # Lists for storing the estimates produced in each iteration\n",
    "        Estimate_X_vec_alpha = []\n",
    "        Estimate_X_discrete_vec_alpha = []\n",
    "        Estimate_Y_vec_alpha = []\n",
    "        Estimate_Y_discrete_vec_alpha = []\n",
    "\n",
    "        # The following while loop generates vanilla and friendship paradox based MLEs in each iteration\n",
    "        while i < No_iterations:\n",
    "            print('n = ' + str(n))  # Number of samples drawn from the degree distribution\n",
    "            print('alpha = ' + str(alpha))  # value of alpha\n",
    "            print('i = ' + str(i))  # Iteration number\n",
    "\n",
    "            # Generating degree sequence by sampling from continuous power-distribution and,\n",
    "            #  then rounding to the nearest integer.\n",
    "            deg_sequence = pareto.rvs(alpha - 1, loc=0, scale=k_min, size=No_of_Nodes)\n",
    "            deg_sequence = np.round(deg_sequence, decimals=0)\n",
    "\n",
    "            #Converting the degree sequence to an array of integers\n",
    "            deg_sequence = np.array(deg_sequence)    \n",
    "            deg_sequence = deg_sequence.astype(int)\n",
    "\n",
    "            # The degrees (d(X_1), d(X_2),....d(X_n)) of the n random nodes\n",
    "            #  and the degrees (d(Y_1), d(Y_2),....d(Y_n)) of n random friends are stored as lists X and Y respectively\n",
    "            X = np.array(np.random.choice(deg_sequence, size=n))\n",
    "            Y = np.array(np.random.choice(deg_sequence, size=n, p=deg_sequence/np.sum(deg_sequence)))\n",
    "\n",
    "            # Computing the vanilla MLE and storing it\n",
    "            alpha_MLE_X = (n/np.sum(np.log(X/(k_min)))) + 1\n",
    "            Estimate_X_vec_alpha.append(alpha_MLE_X)\n",
    "            \n",
    "            # Computing the vanilla discrete MLE and storing it\n",
    "            alpha_MLE_X_discrete = (n/np.sum(np.log(X/(k_min-0.5)))) + 1\n",
    "            Estimate_X_discrete_vec_alpha.append(alpha_MLE_X_discrete)\n",
    "            \n",
    "            # Computing the friendship paradox based MLE and storing it\n",
    "            alpha_MLE_Y = (n/np.sum(np.log(Y/(k_min)))) + 2\n",
    "            Estimate_Y_vec_alpha.append(alpha_MLE_Y)\n",
    "\n",
    "            # Computing the friendship paradox based discrete MLE and storing it\n",
    "            alpha_MLE_Y_discrete = (n/np.sum(np.log(Y/(k_min-0.5)))) + 2\n",
    "            Estimate_Y_discrete_vec_alpha.append(alpha_MLE_Y_discrete)\n",
    "            i = i + 1\n",
    "\n",
    "        # Computing the squared bias, variance and MSE of the vanilla and friendship paradox based MLEs\n",
    "        # that were generated from the while loop  (for the considered value of alpha)\n",
    "        Bias2_Var_MSE_X_vec.append((alpha,\n",
    "                                    (np.mean(Estimate_X_vec_alpha) - alpha),\n",
    "                                    np.mean((Estimate_X_vec_alpha-np.mean(Estimate_X_vec_alpha))**2),\n",
    "                                    np.mean((Estimate_X_vec_alpha-alpha)**2)))\n",
    "        \n",
    "        Bias2_Var_MSE_X_discrete_vec.append((alpha,\n",
    "                            (np.mean(Estimate_X_discrete_vec_alpha) - alpha),\n",
    "                            np.mean((Estimate_X_discrete_vec_alpha-np.mean(Estimate_X_discrete_vec_alpha))**2),\n",
    "                            np.mean((Estimate_X_discrete_vec_alpha-alpha)**2)))\n",
    "\n",
    "        Bias2_Var_MSE_Y_vec.append((alpha,\n",
    "                                    (np.mean(Estimate_Y_vec_alpha) - alpha),\n",
    "                                    np.mean((Estimate_Y_vec_alpha - np.mean(Estimate_Y_vec_alpha))**2),\n",
    "                                    np.mean((Estimate_Y_vec_alpha - alpha)** 2)))\n",
    "        \n",
    "        Bias2_Var_MSE_Y_discrete_vec.append((alpha,\n",
    "                                    (np.mean(Estimate_Y_discrete_vec_alpha) - alpha),\n",
    "                                    np.mean((Estimate_Y_discrete_vec_alpha - np.mean(Estimate_Y_discrete_vec_alpha))**2),\n",
    "                                    np.mean((Estimate_Y_discrete_vec_alpha - alpha)**2)))\n",
    "\n",
    "    #Storing the values in a dictionary in compact form\n",
    "    Dict_Results[n] = (Bias2_Var_MSE_X_vec, Bias2_Var_MSE_X_discrete_vec, Bias2_Var_MSE_Y_vec, Bias2_Var_MSE_Y_discrete_vec)\n",
    "    \n",
    "    \n",
    "    filename_save = 'Var_MSE_CRLB_kmin' + str(k_min) + '.p'\n",
    "    #Saving the results dictionary\n",
    "    try:\n",
    "        import cPickle as pickle\n",
    "    except ImportError:  # python 3.x\n",
    "        import pickle\n",
    "\n",
    "    with open(filename_save, 'wb') as fp:\n",
    "        pickle.dump(Dict_Results, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code generates the Figure 3 of the manuscript (comparison of bias, variance, MSE for sample size $n = 100$)\n",
    "\n",
    "### Description of the Code: This code generates Fig. 3 of the paper to compare the estimate proposed in the paper (friendship paradox based maximum likelihood estimate) with the widely used vanilla method (maximum likelihood estimation with uniform sampling)  in terms of empirically estimated bias, variance and MSE for sample size $n = 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_min_retrieve_list = [1, 5, 50, 500]\n",
    "n = 100\n",
    "\n",
    "# Setting up the parameters of the plot\n",
    "plt.rc('text', usetex=True)\n",
    "MarkerSize = 6\n",
    "MarkerEdgeWidth = 0.6\n",
    "LineWidth = 1.5\n",
    "alpha = 1\n",
    "fig, ax = plt.subplots(nrows=len(k_min_retrieve_list), ncols=3, figsize=(5.49, 5.75))\n",
    "plt.xticks(fontsize=2)\n",
    "\n",
    "row = 0\n",
    "for k_min in k_min_retrieve_list:\n",
    "    \n",
    "    filename_retrieve = 'Var_MSE_CRLB_kmin' + str(k_min) + '.p'\n",
    "\n",
    "    #Loading the results dictionary\n",
    "    with open(filename_retrieve, 'rb') as fp:\n",
    "        data = pickle.load(fp)\n",
    "    \n",
    "    # Storing the squared bias, variance and MSE in lists for considered values of alpha\n",
    "    Alpha_vec, Bias_X, Var_X, MSE_X = zip(*data[n][0])\n",
    "    Alpha_vec, Bias_X_discrete, Var_X_discrete, MSE_X_discrete = zip(*data[n][1])\n",
    "    Alpha_vec, Bias_Y, Var_Y, MSE_Y = zip(*data[n][2])\n",
    "    Alpha_vec, Bias_Y_discrete, Var_Y_discrete, MSE_Y_discrete = zip(*data[n][3])\n",
    "\n",
    "    # Plotting the Bias\n",
    "    ax[row][0].plot(Alpha_vec, Bias_X,\n",
    "                          linestyle='--',\n",
    "                          dashes=(1, 1),\n",
    "                          marker='1',\n",
    "                          markerfacecolor='none',\n",
    "                          markeredgecolor='k',\n",
    "                          c='k',\n",
    "                          alpha = alpha,                        \n",
    "                          linewidth=LineWidth,\n",
    "                          markersize=MarkerSize,\n",
    "                          markeredgewidth=MarkerEdgeWidth,                                        \n",
    "              )\n",
    "\n",
    "    ax[row][0].plot(Alpha_vec, Bias_X_discrete,\n",
    "                          linestyle='--',\n",
    "                          dashes=(1, 1),\n",
    "                          marker='2',\n",
    "                          markerfacecolor='none',\n",
    "                          markeredgecolor='g',\n",
    "                          c='g',\n",
    "                          alpha = alpha,                                            \n",
    "                          linewidth=LineWidth,\n",
    "                          markersize=MarkerSize,\n",
    "                          markeredgewidth=MarkerEdgeWidth,                                        \n",
    "              )\n",
    "\n",
    "    ax[row][0].plot(Alpha_vec, Bias_Y,\n",
    "                          linestyle='--',\n",
    "                          dashes=(1, 1),\n",
    "                          marker='3',\n",
    "                          markerfacecolor='none',\n",
    "                          markeredgecolor='y',\n",
    "                          c='y',\n",
    "                          alpha = alpha,                                            \n",
    "                          linewidth=LineWidth,\n",
    "                          markersize=MarkerSize+0.5,\n",
    "                          markeredgewidth=MarkerEdgeWidth,                                        \n",
    "              )\n",
    "\n",
    "    ax[row][0].plot(Alpha_vec, Bias_Y_discrete,\n",
    "                          linestyle='--',\n",
    "                          dashes=(1, 1),\n",
    "                          marker='4',\n",
    "                          markerfacecolor='none',\n",
    "                          markeredgecolor='r',\n",
    "                          c='r',\n",
    "                          alpha = alpha,                                            \n",
    "                          linewidth=LineWidth,\n",
    "                          markersize=MarkerSize-0.5,\n",
    "                          markeredgewidth=MarkerEdgeWidth,                                        \n",
    "              )\n",
    "\n",
    "\n",
    "    # Plotting the Variance\n",
    "    ax[row][1].plot(Alpha_vec, Var_X,\n",
    "                          linestyle='--',\n",
    "                          dashes=(1, 1),\n",
    "                          marker='1',\n",
    "                          markerfacecolor='none',\n",
    "                          markeredgecolor='k',\n",
    "                          c='k',\n",
    "                          alpha = alpha,                                            \n",
    "                          linewidth=LineWidth,\n",
    "                          markersize=MarkerSize,\n",
    "                          markeredgewidth=MarkerEdgeWidth,                                        \n",
    "                          label=r'$\\hat{\\alpha}_{\\mathrm{vl}}$'           \n",
    "              )\n",
    "\n",
    "    ax[row][1].plot(Alpha_vec, Var_X_discrete,\n",
    "                          linestyle='--',\n",
    "                          dashes=(1, 1),\n",
    "                          marker='2',\n",
    "                          markerfacecolor='none',\n",
    "                          markeredgecolor='g',\n",
    "                          c='g',\n",
    "                          alpha = alpha,                                            \n",
    "                          linewidth=LineWidth,\n",
    "                          markersize=MarkerSize,\n",
    "                          markeredgewidth=MarkerEdgeWidth,                                        \n",
    "                          label=r'$\\hat{\\alpha}_{\\mathrm{vl-d}}$'           \n",
    "              )\n",
    "\n",
    "    ax[row][1].plot(Alpha_vec, Var_Y,\n",
    "                          linestyle='--',\n",
    "                          dashes=(1, 1),\n",
    "                          marker='3',\n",
    "                          markerfacecolor='none',\n",
    "                          markeredgecolor='y',\n",
    "                          c='y',\n",
    "                          alpha = alpha,                                            \n",
    "                          linewidth=LineWidth,\n",
    "                          markersize=MarkerSize+0.5,\n",
    "                          markeredgewidth=MarkerEdgeWidth,                                        \n",
    "                          label=r'$\\hat{\\alpha}_{\\mathrm{fp}}$'           \n",
    "              )\n",
    "\n",
    "    ax[row][1].plot(Alpha_vec, Var_Y_discrete,\n",
    "                          linestyle='--',\n",
    "                          dashes=(1, 1),\n",
    "                          marker='4',\n",
    "                          markerfacecolor='none',\n",
    "                          markeredgecolor='r',\n",
    "                          c='r',\n",
    "                          alpha = alpha,                                            \n",
    "                          linewidth=LineWidth,\n",
    "                          markersize=MarkerSize-0.5,\n",
    "                          markeredgewidth=MarkerEdgeWidth,                                        \n",
    "                          label=r'$\\hat{\\alpha}_{\\mathrm{fp-d}}$'\n",
    "              )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Computing and plotting the theoretical Cramer-Rao Lower Bound values of the vanilla MLE\n",
    "    # against the considered values of alpha\n",
    "    CRLB_X = (np.array(Alpha_vec)-1)**2/n\n",
    "    ax[row][1].plot(Alpha_vec, CRLB_X,\n",
    "                           linestyle='--',\n",
    "                           dashes=(3, 1, 1, 1),\n",
    "                           marker='',\n",
    "                           markerfacecolor='none',\n",
    "                           markeredgecolor='b',\n",
    "                           c='b',\n",
    "                           alpha = alpha,                                            \n",
    "                           linewidth=LineWidth-0.2,\n",
    "                           markersize=MarkerSize,\n",
    "                           markeredgewidth=MarkerEdgeWidth,                                        \n",
    "                           label=r'${\\mathrm{CRLB}}_{\\mathrm{vl}}(\\alpha)$'\n",
    "              )\n",
    "\n",
    "    # Computing and plotting the theoretical Cramer-Rao Lower Bound values of the friendship paradox based MLE\n",
    "    # against the considered values of alpha\n",
    "    CRLB_Y = (np.array(Alpha_vec)-2)**2/n\n",
    "    ax[row][1].plot(Alpha_vec, CRLB_Y,\n",
    "                           linestyle=':',\n",
    "                           dashes=(1, 1),\n",
    "                           marker='',\n",
    "                           markerfacecolor='none',\n",
    "                           markeredgecolor='b',\n",
    "                           c='b',\n",
    "                           alpha = alpha,                                            \n",
    "                           linewidth=LineWidth-0.2,\n",
    "                           markersize=MarkerSize,\n",
    "                           markeredgewidth=MarkerEdgeWidth,                    \n",
    "                           label=r'${\\mathrm{CRLB}}_{\\mathrm{fp}}(\\alpha)$'\n",
    "              )\n",
    "\n",
    "\n",
    "    # Plotting the MSE\n",
    "    ax[row][2].plot(Alpha_vec, MSE_X,\n",
    "                          linestyle='--',\n",
    "                          dashes=(1, 1),\n",
    "                          marker='1',\n",
    "                          markerfacecolor='none',\n",
    "                          markeredgecolor='k',\n",
    "                          c='k',\n",
    "                          alpha = alpha,                                            \n",
    "                          linewidth=LineWidth,\n",
    "                          markersize=MarkerSize,\n",
    "                          markeredgewidth=MarkerEdgeWidth,                                        \n",
    "              )\n",
    "\n",
    "    ax[row][2].plot(Alpha_vec, MSE_X_discrete,\n",
    "                          linestyle='--',\n",
    "                          dashes=(1, 1),\n",
    "                          marker='2',\n",
    "                          markerfacecolor='none',\n",
    "                          markeredgecolor='g',\n",
    "                          c='g',\n",
    "                          alpha = alpha,                                            \n",
    "                          linewidth=LineWidth,\n",
    "                          markersize=MarkerSize,\n",
    "                          markeredgewidth=MarkerEdgeWidth,                                        \n",
    "              )\n",
    "\n",
    "    ax[row][2].plot(Alpha_vec, MSE_Y,\n",
    "                          linestyle='--',\n",
    "                          dashes=(1, 1),\n",
    "                          marker='3',\n",
    "                          markerfacecolor='none',\n",
    "                          markeredgecolor='y',\n",
    "                          c='y',\n",
    "                          alpha = alpha,                                            \n",
    "                          linewidth=LineWidth,\n",
    "                          markersize=MarkerSize+0.5,\n",
    "                          markeredgewidth=MarkerEdgeWidth,                                        \n",
    "              )\n",
    "\n",
    "    ax[row][2].plot(Alpha_vec, MSE_Y_discrete,\n",
    "                          linestyle='--',\n",
    "                          dashes=(1, 1),\n",
    "                          marker='4',\n",
    "                          markerfacecolor='none',\n",
    "                          markeredgecolor='r',\n",
    "                          c='r',\n",
    "                          alpha = alpha,                                            \n",
    "                          linewidth=LineWidth,\n",
    "                          markersize=MarkerSize-0.5,\n",
    "                          markeredgewidth=MarkerEdgeWidth,                    \n",
    "              )\n",
    "\n",
    "    row+=1 \n",
    "\n",
    "for r in np.arange(0,len(k_min_retrieve_list),1):\n",
    "    for i in [0,1,2]:\n",
    "        if i == 0:\n",
    "            ax[r][i].set_ylabel(r'Bias', fontsize=7,labelpad=2)\n",
    "        if i == 1:\n",
    "            ax[r][i].set_ylabel(r'Variance', fontsize=7,labelpad=2)\n",
    "            handles, labels = ax[r][i].get_legend_handles_labels()\n",
    "        if i == 2:\n",
    "            ax[r][i].set_ylabel(r'MSE', fontsize=7,labelpad=2)\n",
    "\n",
    "        ax[r][i].set_xlabel(r'Power-law exponent $\\alpha$', fontsize=7,labelpad=2)\n",
    "        ax[r][i].set_xticks([2.1, 2.5, 3.0, 3.5])        \n",
    "        ax[r][i].tick_params(axis='both', which='major', labelsize=5)\n",
    "        ax[r][i].set_title(r'$k_{min} = \\,\\,$' + str(k_min_retrieve_list[r]),  fontsize=7, pad=3)  \n",
    "\n",
    "plt.subplots_adjust(bottom = 0.08, wspace=0.4, hspace=0.6)        \n",
    "fig.legend(handles, labels, loc='center', ncol=3, fontsize=10, bbox_to_anchor=(0.5, +0.95), edgecolor = 'none')        \n",
    "\n",
    "plt.savefig('Var_MSE_CRLB.pdf', bbox_inches='tight', pad_inches=0.14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Figure 4 (MSE for Different sample Sizes)\n",
    "\n",
    "### Description of the Code: This code generates the Figure 4 of the paper titled \"Estimating Power-law Degree Distributions via Friendship Paradox Sampling\". It compares the estimate proposed in the paper (friendship paradox based maximum likelihood estimate) with the widely used vanilla method (maximum likelihood estimation with uniform sampling) in terms of the mean-squared error under four different sample sizes (n =  50, 100, 150, 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_min_retrieve_list = [1,5,50]\n",
    "n = 100\n",
    "\n",
    "# Setting up the parameters of the plot\n",
    "fig, ax = plt.subplots(nrows = 1, ncols=len(k_min_retrieve_list), figsize=(5.49, 2.2))\n",
    "fig.text(0.515, -0.05, r'Power-law exponent $\\alpha$', fontsize=7, ha='center')\n",
    "fig.text(0.04, 0.5, r'MSE', fontsize=7, rotation='vertical')\n",
    "\n",
    "i = 0\n",
    "for k_min in k_min_retrieve_list:\n",
    "    \n",
    "    filename_retrieve = 'Var_MSE_CRLB_kmin' + str(k_min) + '.p'\n",
    "\n",
    "    #Loading the results dictionary\n",
    "    with open(filename_retrieve, 'rb') as fp:\n",
    "        data = pickle.load(fp)\n",
    "\n",
    "    # Setting up the parameters of the plot\n",
    "    plt.rc('text', usetex=True)\n",
    "#     Marker_List = [6,7,8,9]\n",
    "    Marker_List = ['x','+','s','o']    \n",
    "\n",
    "    \n",
    "    row = 0\n",
    "    j = 0\n",
    "    for n in [50,100,150,200]:\n",
    "\n",
    "\n",
    "        # Storing the squared bias, variance and MSE in lists for considered values of alpha\n",
    "        Alpha_vec, Bias_X, Var_X, MSE_X = zip(*data[n][0])\n",
    "        Alpha_vec, Bias_X_discrete, Var_X_discrete, MSE_X_discrete = zip(*data[n][1])\n",
    "        Alpha_vec, Bias_Y, Var_Y, MSE_Y = zip(*data[n][2])\n",
    "        Alpha_vec, Bias_Y_discrete, Var_Y_discrete, MSE_Y_discrete = zip(*data[n][3])\n",
    "\n",
    "\n",
    "        # Setting the parameters of the plots\n",
    "        plt.rc('text', usetex=True) \n",
    "        MarkerEdgeWidth = 0.4\n",
    "        LineWidth = 1\n",
    "        alpha = 0.7\n",
    "#         MarkerSize = 4\n",
    "        if n == 50:\n",
    "            MarkerSize = 4\n",
    "        if n == 100:\n",
    "            MarkerSize = 4\n",
    "        if n == 150:\n",
    "            MarkerSize = 4\n",
    "        if n == 200:\n",
    "            MarkerSize = 4                   \n",
    "\n",
    "        # Plotting the values of MSE of the vanilla and friendship paradox based MLEs against the\n",
    "        # considered values of alpha and sample size\n",
    "        plot_MSE_X = ax[i].plot(Alpha_vec, MSE_X,\n",
    "                              linestyle=':',\n",
    "    #                           dashes=(6 - j, 1+j),\n",
    "                              marker=Marker_List[j],\n",
    "    #                           alpha=alpha - (j * 0.05),\n",
    "                              markerfacecolor='none',\n",
    "    #                           markeredgecolor=line_colors[j],\n",
    "                              c='k',\n",
    "                              linewidth=LineWidth,\n",
    "                              markersize=MarkerSize,\n",
    "                              markeredgewidth=MarkerEdgeWidth,                                \n",
    "                              label=r'${\\hat{\\alpha}_{\\mathrm{vl}},\\, n = }$' + str(n))\n",
    "\n",
    "        plot_MSE_X_discrete = ax[i].plot(Alpha_vec, MSE_X_discrete,\n",
    "                              linestyle=':',\n",
    "    #                           dashes=(6 - j, 1+j),\n",
    "                              marker=Marker_List[j],\n",
    "    #                           alpha=alpha - (j * 0.05),\n",
    "                              markerfacecolor='none',\n",
    "    #                           markeredgecolor=line_colors[j],\n",
    "                              c='g',\n",
    "                              linewidth=LineWidth,\n",
    "                              markersize=MarkerSize,\n",
    "                              markeredgewidth=MarkerEdgeWidth,                                         \n",
    "                              label=r'${\\hat{\\alpha}_{\\mathrm{vl-d}},\\, n = }$' + str(n))    \n",
    "\n",
    "        plot_MSE_Y = ax[i].plot(Alpha_vec, MSE_Y,\n",
    "                              linestyle=':',\n",
    "    #                           dashes=(6 - j, 1+j),\n",
    "                              marker=Marker_List[j],\n",
    "    #                           alpha=alpha - (j * 0.075),\n",
    "                              markerfacecolor='none',\n",
    "    #                           markeredgecolor=line_colors[j],\n",
    "                              c='y',\n",
    "                              linewidth=LineWidth,\n",
    "                              markersize=MarkerSize,\n",
    "                              markeredgewidth=MarkerEdgeWidth,\n",
    "                              label=r'${\\hat{\\alpha}_{\\mathrm{fp}},\\, n = }$' + str(n))\n",
    "\n",
    "\n",
    "        plot_MSE_Y_discrete = ax[i].plot(Alpha_vec, MSE_Y_discrete,\n",
    "                              linestyle=':',\n",
    "    #                           dashes=(6 - j, 1+j),\n",
    "                              marker=Marker_List[j],\n",
    "    #                           alpha=alpha - (j * 0.075),\n",
    "                              markerfacecolor='none',\n",
    "    #                           markeredgecolor=line_colors[j],\n",
    "                              c='r',\n",
    "                              linewidth=LineWidth,\n",
    "                              markersize=MarkerSize,\n",
    "                              markeredgewidth=MarkerEdgeWidth,\n",
    "                              label=r'${\\hat{\\alpha}_{\\mathrm{fp-d}},\\, n = }$' + str(n))\n",
    "\n",
    "\n",
    "        handles, labels = ax[i].get_legend_handles_labels()\n",
    "\n",
    "        j = j+1\n",
    "        \n",
    "    i+=1        \n",
    "        \n",
    "for i in np.arange(0,len(k_min_retrieve_list)):\n",
    "    # Setting up the parameters of the plot and saving\n",
    "#     ax[i].set_xlabel(r'Power-law coefficient $\\alpha$', fontsize=7)\n",
    "#     ax[i].set_ylabel(r'MSE', fontsize=7)\n",
    "    ax[i].set_xticks([2.1, 2.5, 3.0, 3.5])\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax[i].set_title(r'$k_{min} = \\,\\,$' + str(k_min_retrieve_list[i]),  fontsize=10, pad=4)  \n",
    "    ax[i].set_yscale('log')\n",
    "    \n",
    "    \n",
    "fig.legend(handles, labels, loc='center', ncol=4, fontsize=7, bbox_to_anchor=(0.48, +1.25), edgecolor = 'none')            \n",
    "plt.subplots_adjust(bottom = 0.08, wspace=0.38, hspace=0.55)     \n",
    "plt.savefig('SampleSize_vs_MSE.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for comparing the MSE of maximum likelihood methods wth the Least Squares Estimation Methods\n",
    "\n",
    "### Description of the Code: This code generates an additional figure to compare the estimate proposed in the paper (friendship paradox based maximum likelihood estimate), the widely used vanilla method (maximum likelihood estimation with uniform sampling) and the least squares methods for different values of the sample size (n =  50, 100, 150, 200) and minimum degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluting the bias, variance, MSE values of the LS-estimates for difference sample sizes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "random.seed(123)  # Initializing the random number generators\n",
    "\n",
    "No_of_Nodes = 50000  # Number of nodes in each of the power-law graphs\n",
    "No_iterations = 5000  # Number of independent iterations in the Monte-Carlo simulation\n",
    "k_min = 50 # Minimum degree\n",
    "\n",
    "#Dictionary for storing the results for various n values\n",
    "Dict_Results = {}\n",
    "\n",
    "# The following for loop considers four different values of the sample size\n",
    "for n in np.arange(50, 250, 50):\n",
    "    # Lists for storing the bias and variance for each value of alpha for a given value of sample size n\n",
    "    Bias2_Var_MSE_X_vec = []\n",
    "    Bias2_Var_MSE_X_discrete_vec = []\n",
    "    Bias2_Var_MSE_Y_vec = []\n",
    "    Bias2_Var_MSE_Y_discrete_vec = []\n",
    "    \n",
    "    for alpha in np.linspace(2.1, 3.5, num=25):\n",
    "        i = 1  # i is the current iteration number\n",
    "\n",
    "        # Lists for storing the estimates produced in each iteration\n",
    "        Estimate_LS_X_vec_alpha = []\n",
    "        Estimate_LS_Y_vec_alpha = []\n",
    "\n",
    "        # The following while loop generates vanilla and friendship paradox based MLEs in each iteration\n",
    "        while i < No_iterations:\n",
    "            print('n = ' + str(n))  # Number of samples drawn from the degree distribution\n",
    "            print('alpha = ' + str(alpha))  # value of alpha\n",
    "            print('i = ' + str(i))  # Iteration number\n",
    "\n",
    "            # Generating degree sequence by sampling from continuous power-distribution and,\n",
    "            #  then rounding to the nearest integer.\n",
    "            deg_sequence = pareto.rvs(alpha - 1, loc=0, scale=k_min, size=No_of_Nodes)\n",
    "            deg_sequence = np.round(deg_sequence, decimals=0)\n",
    "\n",
    "            #Converting the degree sequence to an array of integers\n",
    "            deg_sequence = np.array(deg_sequence)    \n",
    "            deg_sequence = deg_sequence.astype(int)\n",
    "\n",
    "            # The degrees (d(X_1), d(X_2),....d(X_n)) of the n random nodes\n",
    "            #  and the degrees (d(Y_1), d(Y_2),....d(Y_n)) of n random friends are stored as lists X and Y respectively\n",
    "            X = np.array(np.random.choice(deg_sequence, size=n))\n",
    "            Y = np.array(np.random.choice(deg_sequence, size=n, p=deg_sequence/np.sum(deg_sequence)))\n",
    "\n",
    "            # The degree histograms of random nodes and random friends\n",
    "            p_X = collections.Counter(X)\n",
    "            q_Y = collections.Counter(Y)\n",
    "            NORM_CONST_X = sum(p_X.values())\n",
    "            NORM_CONST_Y = sum(q_Y.values())            \n",
    "            \n",
    "            # Computing the vanilla LS estimate and storing it\n",
    "            \n",
    "            LS_reg_X = LinearRegression().fit( [[-np.log(t)] for t in p_X.keys()], [np.log(t/NORM_CONST_X) for t in p_X.values()])\n",
    "            alpha_LS_X = LS_reg_X.coef_[0]\n",
    "            Estimate_LS_X_vec_alpha.append(alpha_LS_X)\n",
    "\n",
    "            # Computing the FP based LS estimate and storing it\n",
    "            LS_reg_Y = LinearRegression().fit( [[-np.log(t)] for t in q_Y.keys()], [np.log(t/NORM_CONST_Y) for t in q_Y.values()])\n",
    "            alpha_LS_Y = LS_reg_Y.coef_[0]\n",
    "            Estimate_LS_Y_vec_alpha.append(alpha_LS_Y)\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        # Computing the squared bias, variance and MSE of the vanilla and friendship paradox based LS estimates\n",
    "        # that were generated from the while loop  (for the considered value of alpha)\n",
    "        Bias2_Var_MSE_X_vec.append((alpha,\n",
    "                                    (np.mean(Estimate_LS_X_vec_alpha) - alpha),\n",
    "                                    np.mean((Estimate_LS_X_vec_alpha-np.mean(Estimate_LS_X_vec_alpha))**2),\n",
    "                                    np.mean((Estimate_LS_X_vec_alpha-alpha)**2)))\n",
    "\n",
    "        Bias2_Var_MSE_Y_vec.append((alpha,\n",
    "                                    (np.mean(Estimate_LS_Y_vec_alpha) - alpha),\n",
    "                                    np.mean((Estimate_LS_Y_vec_alpha - np.mean(Estimate_LS_Y_vec_alpha))**2),\n",
    "                                    np.mean((Estimate_LS_Y_vec_alpha - alpha)** 2)))\n",
    "        \n",
    "    #Storing the values in a dictionary in compact form\n",
    "    Dict_Results[n] = (Bias2_Var_MSE_X_vec, Bias2_Var_MSE_Y_vec)\n",
    "    \n",
    "    \n",
    "    filename_save = 'LS_Method_Var_MSE_CRLB_kmin' + str(k_min) + '.p'\n",
    "    #Saving the results dictionary\n",
    "    try:\n",
    "        import cPickle as pickle\n",
    "    except ImportError:  # python 3.x\n",
    "        import pickle\n",
    "\n",
    "    with open(filename_save, 'wb') as fp:\n",
    "        pickle.dump(Dict_Results, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_min_retrieve_list = [1,5,50]\n",
    "\n",
    "# Setting up the parameters of the plot\n",
    "fig, ax = plt.subplots(nrows = 1, ncols=len(k_min_retrieve_list), figsize=(5.49, 2.2))\n",
    "fig.text(0.515, -0.05, r'Power-law exponent $\\alpha$', fontsize=7, ha='center')\n",
    "fig.text(0.04, 0.5, r'MSE', fontsize=7, rotation='vertical')\n",
    "\n",
    "i = 0\n",
    "for k_min in k_min_retrieve_list:\n",
    "    \n",
    "    filename_retrieve = 'Var_MSE_CRLB_kmin' + str(k_min) + '.p'\n",
    "    #Loading the results dictionary\n",
    "    with open(filename_retrieve, 'rb') as fp:\n",
    "        data = pickle.load(fp)\n",
    "        \n",
    "    LS_filename_retrieve = 'LS_Method_Var_MSE_CRLB_kmin' + str(k_min) + '.p'\n",
    "    #Loading the results dictionary\n",
    "    with open(LS_filename_retrieve, 'rb') as fp:\n",
    "        LS_data = pickle.load(fp)        \n",
    "        \n",
    "\n",
    "    # Setting up the parameters of the plot\n",
    "    plt.rc('text', usetex=True)\n",
    "    Marker_List = ['x','+','s','o']    \n",
    "\n",
    "    \n",
    "    row = 0\n",
    "    j = 0\n",
    "    for n in [50,100,150,200]:\n",
    "\n",
    "\n",
    "        # Storing the squared bias, variance and MSE in lists for considered values of alpha\n",
    "        Alpha_vec, Bias_X, Var_X, MSE_X = zip(*data[n][0])\n",
    "        Alpha_vec, Bias_X_discrete, Var_X_discrete, MSE_X_discrete = zip(*data[n][1])\n",
    "        Alpha_vec, Bias_Y, Var_Y, MSE_Y = zip(*data[n][2])\n",
    "        Alpha_vec, Bias_Y_discrete, Var_Y_discrete, MSE_Y_discrete = zip(*data[n][3])\n",
    "        \n",
    "        # Storing the squared bias, variance and MSE of LS estimate in lists for considered values of alpha\n",
    "        LS_Alpha_vec, LS_Bias_X, LS_Var_X, LS_MSE_X = zip(*LS_data[n][0])\n",
    "        LS_Alpha_vec, LS_Bias_Y, LS_Var_Y, LS_MSE_Y = zip(*LS_data[n][1])\n",
    "\n",
    "\n",
    "        # Setting the parameters of the plots\n",
    "        plt.rc('text', usetex=True) \n",
    "        MarkerEdgeWidth = 0.4\n",
    "        LineWidth = 0.7\n",
    "        alpha = 1\n",
    "#         MarkerSize = 4\n",
    "        if n == 50:\n",
    "            MarkerSize = 4\n",
    "        if n == 100:\n",
    "            MarkerSize = 4\n",
    "        if n == 150:\n",
    "            MarkerSize = 4\n",
    "        if n == 200:\n",
    "            MarkerSize = 4                   \n",
    "\n",
    "\n",
    "        # Plotting the values of MSE of the vanilla and friendship paradox based MLEs against the\n",
    "        # considered values of alpha and sample size\n",
    "        plot_MSE_X = ax[i].plot(Alpha_vec, MSE_X,\n",
    "                              linestyle=':',\n",
    "                              marker=Marker_List[j],\n",
    "                              markerfacecolor='none',\n",
    "                              c='k',\n",
    "                              linewidth=LineWidth,\n",
    "                              markersize=MarkerSize,\n",
    "                              markeredgewidth=MarkerEdgeWidth,                                \n",
    "                              label=r'${\\hat{\\alpha}_{\\mathrm{vl}},\\, n = }$' + str(n))\n",
    "\n",
    "        plot_MSE_X_discrete = ax[i].plot(Alpha_vec, MSE_X_discrete,\n",
    "                              linestyle=':',\n",
    "                              marker=Marker_List[j],\n",
    "                              markerfacecolor='none',\n",
    "                              c='g',\n",
    "                              linewidth=LineWidth,\n",
    "                              markersize=MarkerSize,\n",
    "                              markeredgewidth=MarkerEdgeWidth,                                         \n",
    "                              label=r'${\\hat{\\alpha}_{\\mathrm{vl-d}},\\, n = }$' + str(n))    \n",
    "\n",
    "        plot_MSE_Y = ax[i].plot(Alpha_vec, MSE_Y,\n",
    "                              linestyle=':',\n",
    "                              marker=Marker_List[j],\n",
    "                              markerfacecolor='none',\n",
    "                              c='y',\n",
    "                              linewidth=LineWidth,\n",
    "                              markersize=MarkerSize,\n",
    "                              markeredgewidth=MarkerEdgeWidth,\n",
    "                              label=r'${\\hat{\\alpha}_{\\mathrm{fp}},\\, n = }$' + str(n))\n",
    "\n",
    "\n",
    "        plot_MSE_Y_discrete = ax[i].plot(Alpha_vec, MSE_Y_discrete,\n",
    "                              linestyle=':',\n",
    "                              marker=Marker_List[j],\n",
    "                              markerfacecolor='none',\n",
    "                              c='r',\n",
    "                              linewidth=LineWidth,\n",
    "                              markersize=MarkerSize,\n",
    "                              markeredgewidth=MarkerEdgeWidth,\n",
    "                              label=r'${\\hat{\\alpha}_{\\mathrm{fp-d}},\\, n = }$' + str(n))\n",
    "\n",
    "        \n",
    "        LS_plot_MSE_X = ax[i].plot(LS_Alpha_vec, LS_MSE_X,\n",
    "                              linestyle=':',\n",
    "                              marker=Marker_List[j],\n",
    "                              markerfacecolor='none',\n",
    "                              c='m',\n",
    "                              linewidth=LineWidth,\n",
    "                              markersize=MarkerSize,\n",
    "                              markeredgewidth=MarkerEdgeWidth,                                \n",
    "                              label=r'${\\hat{\\alpha}_{\\mathrm{vl-LS}},\\, n = }$' + str(n))\n",
    "\n",
    "        \n",
    "        LS_plot_MSE_Y = ax[i].plot(LS_Alpha_vec, LS_MSE_Y,\n",
    "                              linestyle=':',\n",
    "                              marker=Marker_List[j],\n",
    "                              markerfacecolor='none',\n",
    "                              c='c',\n",
    "                              linewidth=LineWidth,\n",
    "                              markersize=MarkerSize,\n",
    "                              markeredgewidth=MarkerEdgeWidth,                                \n",
    "                              label=r'${\\hat{\\alpha}_{\\mathrm{fp-LS}},\\, n = }$' + str(n))        \n",
    "\n",
    "        handles, labels = ax[i].get_legend_handles_labels()\n",
    "\n",
    "        j = j+1\n",
    "        \n",
    "    i+=1        \n",
    "        \n",
    "for i in np.arange(0,len(k_min_retrieve_list)):\n",
    "    # Setting up the parameters of the plot and saving\n",
    "    ax[i].set_xticks([2.1, 2.5, 3.0, 3.5])\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax[i].set_title(r'$k_{min} = \\,\\,$' + str(k_min_retrieve_list[i]),  fontsize=10, pad=4)  \n",
    "    ax[i].set_yscale('log')\n",
    "    \n",
    "    \n",
    "fig.legend(handles, labels, loc='center', ncol=4, fontsize=7, bbox_to_anchor=(0.48, +1.35), edgecolor = 'none')            \n",
    "plt.subplots_adjust(bottom = 0.08, wspace=0.38, hspace=0.55)     \n",
    "plt.savefig('LS_Method_SampleSize_vs_MSE.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
